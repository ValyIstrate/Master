{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-12T13:49:47.922111Z",
     "start_time": "2024-10-12T13:49:47.896696Z"
    }
   },
   "source": [
    "from datetime import datetime\n",
    "from torcheval.metrics.functional import multiclass_accuracy\n",
    "from torcheval.metrics.functional import multiclass_f1_score\n",
    "import gzip\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "class DatasetWrapper:\n",
    "    def __init__(self, train_set_elements, train_set_labels):\n",
    "        self.elements = train_set_elements\n",
    "        self.labels = train_set_labels\n",
    "\n",
    "\n",
    "def get_datasets():\n",
    "    with gzip.open(\"mnist.pkl.gz\", \"rb\") as fd:\n",
    "        initial_train_set, valid_set, initial_test_set = pickle.load(fd, encoding=\"latin\")\n",
    "\n",
    "    # Combined the train set and the test set\n",
    "    train_set_x = torch.tensor(np.concatenate([initial_train_set[0], initial_test_set[0]]), dtype=torch.float32)\n",
    "    train_set_y = torch.tensor(np.concatenate([initial_train_set[1], initial_test_set[1]]), dtype=torch.long)\n",
    "\n",
    "    valid_set_x = torch.tensor(valid_set[0], dtype=torch.float32)\n",
    "    valid_set_y = torch.tensor(valid_set[1], dtype=torch.long)\n",
    "\n",
    "    print(f'Loaded training dataset: {train_set_x.shape}')\n",
    "    print(f'Loaded validation dataset: {valid_set_x.shape}')\n",
    "\n",
    "    return DatasetWrapper(train_set_x, train_set_y), DatasetWrapper(valid_set_x, valid_set_y)\n",
    "\n",
    "\n",
    "def init_kaiming_uniform(tensor, fan_in):\n",
    "    \"\"\"\n",
    "    Because we are using the ReLU activation function, initializing the weights using the\n",
    "    Kaiming uniform method is recommended.\n",
    "    https://ai.stackexchange.com/questions/32247/what-is-the-analytical-formula-for-kaiming-he-probability-density-function\n",
    "    :param tensor: The tensor we want to initialize the weights with.\n",
    "    :param fan_in: Number of input units\n",
    "    :return: The initialized weights\n",
    "    \"\"\"\n",
    "    gain = torch.sqrt(torch.tensor(2.0))\n",
    "    std = gain / torch.sqrt(torch.tensor(fan_in, dtype=torch.float32))\n",
    "    bound = torch.sqrt(torch.tensor(3.0)) * std\n",
    "    with torch.no_grad():\n",
    "        tensor.uniform_(-bound, bound)\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return torch.maximum(x, torch.tensor(0.0))\n",
    "\n",
    "\n",
    "def softmax(x, dim=1):\n",
    "    exps = torch.exp(x - torch.max(x, dim=dim, keepdim=True).values)\n",
    "    return exps / exps.sum(dim=dim, keepdim=True)\n",
    "\n",
    "\n",
    "class DigitClassifier:\n",
    "    def __init__(self):\n",
    "        # These values will be used later\n",
    "        self.output = None\n",
    "        self.hidden_activation = None\n",
    "        self.hidden_output = None\n",
    "        \n",
    "        # Input layer - 784 units\n",
    "        # Hidden Layer - 100 units\n",
    "        self.hidden_weight = torch.zeros(784, 100)\n",
    "        self.hidden_bias = torch.zeros(100)\n",
    "\n",
    "        # Hidden layer - 100 units\n",
    "        # Output layer - 10 units\n",
    "        self.output_weight = torch.zeros(100, 10)\n",
    "        self.output_bias = torch.zeros(10)\n",
    "\n",
    "        # Initialize the weights\n",
    "        init_kaiming_uniform(self.hidden_weight, 784)\n",
    "        init_kaiming_uniform(self.output_weight, 100)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Manual forward propagation\n",
    "        self.hidden_output = x @ self.hidden_weight + self.hidden_bias\n",
    "        # Use ReLU for the hidden layer\n",
    "        self.hidden_activation = relu(self.hidden_output)\n",
    "        self.output = self.hidden_activation @ self.output_weight + self.output_bias\n",
    "        # Use softmax for the output layer\n",
    "        return softmax(self.output, dim=1)\n",
    "\n",
    "\n",
    "    def backward(self, x, y, output):\n",
    "        batch_size = y.size(0)\n",
    "\n",
    "        # Backpropagation through the output layer (softmax + gradient descent)\n",
    "        d_output = output.clone()\n",
    "        d_output[range(batch_size), y] -= 1  # Gradient of softmax and cross-entropy combined\n",
    "        d_output /= batch_size\n",
    "\n",
    "        # Gradients calculated based on output layer weights and biases\n",
    "        dw_output = self.hidden_activation.T @ d_output\n",
    "        db_output = d_output.sum(dim=0)\n",
    "\n",
    "        # Backpropagation through the hidden layer\n",
    "        d_hidden_activation = d_output @ self.output_weight.T\n",
    "        d_hidden_output = d_hidden_activation.clone()\n",
    "        # ReLU derivative: 0 if input <= 0, 1 otherwise\n",
    "        d_hidden_output[self.hidden_output <= 0] = 0\n",
    "\n",
    "        dw_hidden = x.T @ d_hidden_output\n",
    "        db_hidden = d_hidden_output.sum(dim=0)\n",
    "\n",
    "        # Manually set gradients\n",
    "        self.hidden_weight.grad = dw_hidden\n",
    "        self.hidden_bias.grad = db_hidden\n",
    "        self.output_weight.grad = dw_output\n",
    "        self.output_bias.grad = db_output\n",
    "        \n",
    "def write_data_set_scores(prediction, file, dataset_wrapper: DatasetWrapper):\n",
    "    \"\"\"\n",
    "    Method writes the training results in a file.\n",
    "    :param prediction: The predictions on the dataset, at the end of the training loop.\n",
    "    :param file: The file in which the results should be written to.\n",
    "    :param dataset_wrapper: The class containing the dataset elements and labels\n",
    "    :return: Void method\n",
    "    \"\"\"\n",
    "\n",
    "    file.write(f\"Accuracy: {multiclass_accuracy(prediction, dataset_wrapper.labels)}\\n\")\n",
    "    file.write(f\"F1-Score: {multiclass_f1_score(prediction, dataset_wrapper.labels)}\\n\")\n",
    "    file.write(f\"All Classes Accuracy: \\n\")\n",
    "    for cls, value in enumerate(multiclass_accuracy(prediction, dataset_wrapper.labels, average=None, num_classes=10)):\n",
    "        file.write(f\"     {cls}: {value}\\n\")\n",
    "    file.write(f\"All Classes F1-Score: \\n\")\n",
    "    for cls, value in enumerate(multiclass_f1_score(prediction, dataset_wrapper.labels, average=None, num_classes=10)):\n",
    "        file.write(f\"     {cls}: {value}\\n\")\n",
    "\n",
    "\n",
    "def cross_entropy_loss(output, target):\n",
    "    \"\"\"\n",
    "    Method computes the cross entropy loss.\n",
    "    :param output: This is the predicted output from the model, typically after applying softmax\n",
    "    :param target: This is a tensor containing the true class labels (integers) for each sample, of shape.\n",
    "    :return: Returns the cross entropy loss.\n",
    "    \"\"\"\n",
    "    # log_probs = torch.log(output) # This takes the natural logarithm of the predicted probabilities\n",
    "    # return -torch.mean(torch.gather(log_probs, 1, target.unsqueeze(1)))\n",
    "    batch_size = output.size(0)\n",
    "    num_classes = output.size(1)\n",
    "\n",
    "    log_probs = torch.log(output)  #Calculate the logarithm of the predicted probabilities\n",
    "\n",
    "    # Create one-hot encoded target tensor of shape (batch_size, num_classes)\n",
    "    one_hot_target = torch.zeros(batch_size, num_classes).to(output.device)\n",
    "    one_hot_target[range(batch_size), target] = 1\n",
    "\n",
    "    # Calculate the cross-entropy loss using the formula in Course 1\n",
    "    loss = -torch.sum(one_hot_target * log_probs) / batch_size\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train_network():\n",
    "    start = datetime.now()\n",
    "    train_set, validation_set = get_datasets()\n",
    "\n",
    "    lr = 0.005\n",
    "    epochs = 100\n",
    "    batch_size = 32\n",
    "\n",
    "    model = DigitClassifier()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, len(train_set.elements), batch_size):\n",
    "            images = train_set.elements[i: i + batch_size].view(-1, 784)\n",
    "            labels = train_set.labels[i: i + batch_size]\n",
    "            output = model.forward(images)\n",
    "\n",
    "            model.backward(images, labels, output)\n",
    "\n",
    "            # Disable autograd for manual update\n",
    "            with torch.no_grad():\n",
    "                for param in [model.hidden_weight, model.hidden_bias, model.output_weight, model.output_bias]:\n",
    "                    param -= lr * param.grad\n",
    "\n",
    "            # Zero gradients for the next step\n",
    "            for param in [model.hidden_weight, model.hidden_bias, model.output_weight, model.output_bias]:\n",
    "                param.grad.zero_()\n",
    "\n",
    "        print(f'Epoch {epoch + 1} with loss'\n",
    "              f' {cross_entropy_loss(model.forward(train_set.elements.view(-1, 784)), train_set.labels)}')\n",
    "\n",
    "    train_set_file = open(\"train_set.txt\", \"w\")\n",
    "    valid_set_file = open(\"valid_set.txt\", \"w\")\n",
    "    with torch.no_grad():\n",
    "        predicted_train = model.forward(train_set.elements)\n",
    "    write_data_set_scores(predicted_train, train_set_file, train_set)\n",
    "    train_set_file.close()\n",
    "    with torch.no_grad():\n",
    "        predicted_valid = model.forward(validation_set.elements)\n",
    "    write_data_set_scores(predicted_valid, valid_set_file, validation_set)\n",
    "    valid_set_file.close()\n",
    "\n",
    "    end = datetime.now()\n",
    "\n",
    "    print(f'Runtime: {end - start}')\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T13:51:37.682605Z",
     "start_time": "2024-10-12T13:49:59.142387Z"
    }
   },
   "cell_type": "code",
   "source": "train_network()",
   "id": "6db33d800da5d3b6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training dataset: torch.Size([60000, 784])\n",
      "Loaded validation dataset: torch.Size([10000, 784])\n",
      "Epoch 1 with loss 0.4783589839935303\n",
      "Epoch 2 with loss 0.37855955958366394\n",
      "Epoch 3 with loss 0.33716341853141785\n",
      "Epoch 4 with loss 0.31142786145210266\n",
      "Epoch 5 with loss 0.29236891865730286\n",
      "Epoch 6 with loss 0.27691611647605896\n",
      "Epoch 7 with loss 0.2636857330799103\n",
      "Epoch 8 with loss 0.25213858485221863\n",
      "Epoch 9 with loss 0.2416916936635971\n",
      "Epoch 10 with loss 0.2321300059556961\n",
      "Epoch 11 with loss 0.22328853607177734\n",
      "Epoch 12 with loss 0.2150094211101532\n",
      "Epoch 13 with loss 0.20727549493312836\n",
      "Epoch 14 with loss 0.20003801584243774\n",
      "Epoch 15 with loss 0.19321204721927643\n",
      "Epoch 16 with loss 0.18687781691551208\n",
      "Epoch 17 with loss 0.18092043697834015\n",
      "Epoch 18 with loss 0.1753610223531723\n",
      "Epoch 19 with loss 0.17017148435115814\n",
      "Epoch 20 with loss 0.16527734696865082\n",
      "Epoch 21 with loss 0.1606440395116806\n",
      "Epoch 22 with loss 0.15630143880844116\n",
      "Epoch 23 with loss 0.1521693468093872\n",
      "Epoch 24 with loss 0.14826931059360504\n",
      "Epoch 25 with loss 0.14456439018249512\n",
      "Epoch 26 with loss 0.14101839065551758\n",
      "Epoch 27 with loss 0.1376737654209137\n",
      "Epoch 28 with loss 0.13449886441230774\n",
      "Epoch 29 with loss 0.13147635757923126\n",
      "Epoch 30 with loss 0.12858200073242188\n",
      "Epoch 31 with loss 0.1257963925600052\n",
      "Epoch 32 with loss 0.12312807887792587\n",
      "Epoch 33 with loss 0.12056567519903183\n",
      "Epoch 34 with loss 0.11812518537044525\n",
      "Epoch 35 with loss 0.11575575917959213\n",
      "Epoch 36 with loss 0.11349570006132126\n",
      "Epoch 37 with loss 0.11132965236902237\n",
      "Epoch 38 with loss 0.10925150662660599\n",
      "Epoch 39 with loss 0.10725260525941849\n",
      "Epoch 40 with loss 0.10532162338495255\n",
      "Epoch 41 with loss 0.10347172617912292\n",
      "Epoch 42 with loss 0.10167472809553146\n",
      "Epoch 43 with loss 0.09993402659893036\n",
      "Epoch 44 with loss 0.09825493395328522\n",
      "Epoch 45 with loss 0.09662244468927383\n",
      "Epoch 46 with loss 0.09504792839288712\n",
      "Epoch 47 with loss 0.09350714832544327\n",
      "Epoch 48 with loss 0.09202717989683151\n",
      "Epoch 49 with loss 0.09058867394924164\n",
      "Epoch 50 with loss 0.08919943124055862\n",
      "Epoch 51 with loss 0.08784790337085724\n",
      "Epoch 52 with loss 0.08653118461370468\n",
      "Epoch 53 with loss 0.08525559306144714\n",
      "Epoch 54 with loss 0.08402242511510849\n",
      "Epoch 55 with loss 0.08281975984573364\n",
      "Epoch 56 with loss 0.08164310455322266\n",
      "Epoch 57 with loss 0.0804998129606247\n",
      "Epoch 58 with loss 0.07938375324010849\n",
      "Epoch 59 with loss 0.0782993957400322\n",
      "Epoch 60 with loss 0.07724956423044205\n",
      "Epoch 61 with loss 0.07621407508850098\n",
      "Epoch 62 with loss 0.07520356774330139\n",
      "Epoch 63 with loss 0.07421883940696716\n",
      "Epoch 64 with loss 0.073269322514534\n",
      "Epoch 65 with loss 0.07234173268079758\n",
      "Epoch 66 with loss 0.07143080234527588\n",
      "Epoch 67 with loss 0.07054175436496735\n",
      "Epoch 68 with loss 0.0696791261434555\n",
      "Epoch 69 with loss 0.06882815808057785\n",
      "Epoch 70 with loss 0.0680028647184372\n",
      "Epoch 71 with loss 0.0671912133693695\n",
      "Epoch 72 with loss 0.06640215963125229\n",
      "Epoch 73 with loss 0.06562792509794235\n",
      "Epoch 74 with loss 0.06487729400396347\n",
      "Epoch 75 with loss 0.06412582844495773\n",
      "Epoch 76 with loss 0.06340208649635315\n",
      "Epoch 77 with loss 0.06269066035747528\n",
      "Epoch 78 with loss 0.061986785382032394\n",
      "Epoch 79 with loss 0.061301834881305695\n",
      "Epoch 80 with loss 0.060625214129686356\n",
      "Epoch 81 with loss 0.059972118586301804\n",
      "Epoch 82 with loss 0.05932317674160004\n",
      "Epoch 83 with loss 0.05868588015437126\n",
      "Epoch 84 with loss 0.05805834010243416\n",
      "Epoch 85 with loss 0.05744171515107155\n",
      "Epoch 86 with loss 0.056838568300008774\n",
      "Epoch 87 with loss 0.0562499575316906\n",
      "Epoch 88 with loss 0.055658675730228424\n",
      "Epoch 89 with loss 0.05508803203701973\n",
      "Epoch 90 with loss 0.05451662465929985\n",
      "Epoch 91 with loss 0.05397246778011322\n",
      "Epoch 92 with loss 0.0534173920750618\n",
      "Epoch 93 with loss 0.0528852753341198\n",
      "Epoch 94 with loss 0.05234748125076294\n",
      "Epoch 95 with loss 0.051831163465976715\n",
      "Epoch 96 with loss 0.051308613270521164\n",
      "Epoch 97 with loss 0.05081533268094063\n",
      "Epoch 98 with loss 0.05031203106045723\n",
      "Epoch 99 with loss 0.04981903359293938\n",
      "Epoch 100 with loss 0.04933813214302063\n",
      "Runtime: 0:01:38.534736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Master\\AdvancedNeuralNetworks\\homework1\\.venv\\Lib\\site-packages\\torcheval\\metrics\\functional\\classification\\accuracy.py:275: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorAdvancedIndexing.cpp:234.)\n",
      "  num_correct = mask.new_zeros(num_classes).scatter_(0, target, mask, reduce=\"add\")\n"
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
